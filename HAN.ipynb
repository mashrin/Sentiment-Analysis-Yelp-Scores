{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import csv\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "def get_evaluation(y_true, y_prob, list_metrics):\n",
    "    y_pred = np.argmax(y_prob, -1)\n",
    "    output = {}\n",
    "    if 'accuracy' in list_metrics:\n",
    "        output['accuracy'] = metrics.accuracy_score(y_true, y_pred)\n",
    "    if 'loss' in list_metrics:\n",
    "        try:\n",
    "            output['loss'] = metrics.log_loss(y_true, y_prob)\n",
    "        except ValueError:\n",
    "            output['loss'] = -1\n",
    "    if 'confusion_matrix' in list_metrics:\n",
    "        output['confusion_matrix'] = str(metrics.confusion_matrix(y_true, y_pred))\n",
    "    return output\n",
    "\n",
    "def matrix_mul(input, weight, bias=False):\n",
    "    feature_list = []\n",
    "    for feature in input:\n",
    "        feature = torch.mm(feature, weight)\n",
    "        if isinstance(bias, torch.nn.parameter.Parameter):\n",
    "            feature = feature + bias.expand(feature.size()[0], bias.size()[1])\n",
    "        feature = torch.tanh(feature).unsqueeze(0)\n",
    "        feature_list.append(feature)\n",
    "\n",
    "    return torch.cat(feature_list, 0).squeeze()\n",
    "\n",
    "def element_wise_mul(input1, input2):\n",
    "\n",
    "    feature_list = []\n",
    "    for feature_1, feature_2 in zip(input1, input2):\n",
    "        feature_2 = feature_2.unsqueeze(1).expand_as(feature_1)\n",
    "        feature = feature_1 * feature_2\n",
    "        feature_list.append(feature.unsqueeze(0))\n",
    "    output = torch.cat(feature_list, 0)\n",
    "\n",
    "    return torch.sum(output, 0).unsqueeze(0)\n",
    "\n",
    "def get_max_lengths(data_path):\n",
    "    word_length_list = []\n",
    "    sent_length_list = []\n",
    "    with open(data_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, quotechar='\"')\n",
    "        for idx, line in enumerate(reader):\n",
    "            text = \"\"\n",
    "            for tx in line[1:]:\n",
    "                text += tx.lower()\n",
    "                text += \" \"\n",
    "            sent_list = sent_tokenize(text)\n",
    "            sent_length_list.append(len(sent_list))\n",
    "\n",
    "            for sent in sent_list:\n",
    "                word_list = word_tokenize(sent)\n",
    "                word_length_list.append(len(word_list))\n",
    "\n",
    "        sorted_word_length = sorted(word_length_list)\n",
    "        sorted_sent_length = sorted(sent_length_list)\n",
    "\n",
    "    return sorted_word_length[int(0.8*len(sorted_word_length))], sorted_sent_length[int(0.8*len(sorted_sent_length))]\n",
    "\n",
    "word, sent = get_max_lengths(\"../binary_data/test.csv\")\n",
    "print (word)\n",
    "print (sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, dict_path, max_length_sentences=30, max_length_word=50):\n",
    "        super(CustomDataset, self).__init__()\n",
    "\n",
    "        texts, labels = [], []\n",
    "        with open(data_path) as csv_file:\n",
    "            reader = csv.reader(csv_file, quotechar='\"')\n",
    "            next(reader)\n",
    "            for idx, line in enumerate(reader):\n",
    "                text = line[2].lower()\n",
    "                label = int(line[0])\n",
    "                texts.append(text)\n",
    "                labels.append(label)\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.dict = pd.read_csv(filepath_or_buffer=dict_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE,\n",
    "                                usecols=[0]).values\n",
    "        self.dict = [word[0] for word in self.dict]\n",
    "        self.max_length_sentences = max_length_sentences\n",
    "        self.max_length_word = max_length_word\n",
    "        self.num_classes = len(set(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        text = self.texts[index]\n",
    "        document_encode = [\n",
    "            [self.dict.index(word) if word in self.dict else -1 for word in word_tokenize(text=sentences)] for sentences\n",
    "            in\n",
    "            sent_tokenize(text=text)]\n",
    "\n",
    "        for sentences in document_encode:\n",
    "            if len(sentences) < self.max_length_word:\n",
    "                extended_words = [-1 for _ in range(self.max_length_word - len(sentences))]\n",
    "                sentences.extend(extended_words)\n",
    "\n",
    "        if len(document_encode) < self.max_length_sentences:\n",
    "            extended_sentences = [[-1 for _ in range(self.max_length_word)] for _ in\n",
    "                                  range(self.max_length_sentences - len(document_encode))]\n",
    "            document_encode.extend(extended_sentences)\n",
    "\n",
    "        document_encode = [sentences[:self.max_length_word] for sentences in document_encode][\n",
    "                          :self.max_length_sentences]\n",
    "\n",
    "        document_encode = np.stack(arrays=document_encode, axis=0)\n",
    "        document_encode += 1\n",
    "\n",
    "        return document_encode.astype(np.int64), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentAttNet(nn.Module):\n",
    "    def __init__(self, sent_hidden_size=50, word_hidden_size=50, num_classes=2):\n",
    "        super(SentAttNet, self).__init__()\n",
    "\n",
    "        self.sent_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 2 * sent_hidden_size))\n",
    "        self.sent_bias = nn.Parameter(torch.Tensor(1, 2 * sent_hidden_size))\n",
    "        self.context_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 1))\n",
    "\n",
    "        self.lstm = nn.LSTM(2 * word_hidden_size, sent_hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(2 * sent_hidden_size, num_classes)\n",
    "        self._create_weights(mean=0.0, std=0.05)\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "        self.sent_weight.data.normal_(mean, std)\n",
    "        self.context_weight.data.normal_(mean, std)\n",
    "\n",
    "    def forward(self, input, hidden_state):\n",
    "\n",
    "        f_output, h_output = self.lstm(input, hidden_state)\n",
    "        # Attention mechanism\n",
    "        output = matrix_mul(f_output, self.sent_weight, self.sent_bias)\n",
    "        output = matrix_mul(output, self.context_weight).permute(1, 0)\n",
    "        output = F.softmax(output)\n",
    "        output = element_wise_mul(f_output, output.permute(1, 0)).squeeze(0)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, h_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordAttNet(nn.Module):\n",
    "    def __init__(self, word2vec_path, hidden_size=50):\n",
    "        super(WordAttNet, self).__init__()\n",
    "        dict = pd.read_csv(filepath_or_buffer=word2vec_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE).values[:, 1:]\n",
    "        dict_len, embed_size = dict.shape\n",
    "        dict_len += 1\n",
    "        unknown_word = np.zeros((1, embed_size))\n",
    "        dict = torch.from_numpy(np.concatenate([unknown_word, dict], axis=0).astype(np.float))\n",
    "\n",
    "        self.word_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 2 * hidden_size))\n",
    "        self.word_bias = nn.Parameter(torch.Tensor(1, 2 * hidden_size))\n",
    "        self.context_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n",
    "\n",
    "        self.lookup = nn.Embedding(num_embeddings=dict_len, embedding_dim=embed_size).from_pretrained(dict)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n",
    "        self._create_weights(mean=0.0, std=0.05)\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "\n",
    "        self.word_weight.data.normal_(mean, std)\n",
    "        self.context_weight.data.normal_(mean, std)\n",
    "\n",
    "    def forward(self, input, hidden_state):\n",
    "\n",
    "        output = self.lookup(input)\n",
    "        f_output, h_output = self.lstm(output.float(), hidden_state)  \n",
    "        # Attention mechanism\n",
    "        output = matrix_mul(f_output, self.word_weight, self.word_bias)\n",
    "        output = matrix_mul(output, self.context_weight).permute(1,0)\n",
    "        output = F.softmax(output)\n",
    "        output = element_wise_mul(f_output,output.permute(1,0))\n",
    "\n",
    "        return output, h_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierAttNet(nn.Module):\n",
    "    def __init__(self, word_hidden_size, sent_hidden_size, batch_size, num_classes, pretrained_word2vec_path,\n",
    "                 max_sent_length, max_word_length):\n",
    "        super(HierAttNet, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.word_hidden_size = word_hidden_size\n",
    "        self.sent_hidden_size = sent_hidden_size\n",
    "        self.max_sent_length = max_sent_length\n",
    "        self.max_word_length = max_word_length\n",
    "        self.word_att_net = WordAttNet(pretrained_word2vec_path, word_hidden_size)\n",
    "        self.sent_att_net = SentAttNet(sent_hidden_size, word_hidden_size, num_classes)\n",
    "        self._init_hidden_state()\n",
    "\n",
    "    def _init_hidden_state(self, last_batch_size=None):\n",
    "        if last_batch_size:\n",
    "            batch_size = last_batch_size\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "        self.word_hidden_state = torch.zeros(2, batch_size, self.word_hidden_size)\n",
    "        self.sent_hidden_state = torch.zeros(2, batch_size, self.sent_hidden_size)\n",
    "        if torch.cuda.is_available():\n",
    "            self.word_hidden_state = self.word_hidden_state.cuda()\n",
    "            self.sent_hidden_state = self.sent_hidden_state.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output_list = []\n",
    "        input = input.permute(1, 0, 2)\n",
    "        for i in input:\n",
    "            output, self.word_hidden_state = self.word_att_net(i.permute(1, 0), self.word_hidden_state)\n",
    "            output_list.append(output)\n",
    "        output = torch.cat(output_list, 0)\n",
    "        output, self.sent_hidden_state = self.sent_att_net(output, self.sent_hidden_state)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(opt, model):\n",
    "    max_word_length, max_sent_length = get_max_lengths(opt.test_set)\n",
    "    test_params = {\"batch_size\": opt.batch_size,\n",
    "                   \"shuffle\": False,\n",
    "                   \"drop_last\": False}\n",
    "    test_set = CustomDataset(opt.test_set, opt.word2vec_path, max_sent_length, max_word_length)\n",
    "    test_generator = DataLoader(test_set, **test_params)\n",
    "    model.eval()\n",
    "    loss_ls = []\n",
    "    te_label_ls = []\n",
    "    te_pred_ls = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for te_feature, te_label in test_generator:\n",
    "        num_sample = len(te_label)\n",
    "        if torch.cuda.is_available():\n",
    "            te_feature = te_feature.cuda()\n",
    "            te_label = te_label.cuda()\n",
    "        with torch.no_grad():\n",
    "            model._init_hidden_state(num_sample)\n",
    "            te_predictions = model(te_feature)\n",
    "        te_loss = criterion(te_predictions, te_label)\n",
    "        loss_ls.append(te_loss * num_sample)\n",
    "        te_label_ls.extend(te_label.clone().cpu())\n",
    "        te_pred_ls.append(te_predictions.clone().cpu())\n",
    "    te_loss = sum(loss_ls) / test_set.__len__()\n",
    "    te_pred = torch.cat(te_pred_ls, 0)\n",
    "    te_label = np.array(te_label_ls)\n",
    "    test_metrics = get_evaluation(te_label, te_pred.numpy(), list_metrics=[\"accuracy\", \"confusion_matrix\"])\n",
    "    print(\"Test Loss: {}, Test Accuracy: {}\".format(\n",
    "        te_loss, test_metrics[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(opt):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(123)\n",
    "    else:\n",
    "        torch.manual_seed(123)\n",
    "    output_file = open(opt.saved_path + os.sep + \"logs.txt\", \"w\")\n",
    "    output_file.write(\"Model's parameters: {}\".format(vars(opt)))\n",
    "    training_params = {\"batch_size\": opt.batch_size,\n",
    "                       \"shuffle\": True,\n",
    "                       \"drop_last\": True}\n",
    "    test_params = {\"batch_size\": opt.batch_size,\n",
    "                   \"shuffle\": False,\n",
    "                   \"drop_last\": False}\n",
    "\n",
    "    max_word_length, max_sent_length = get_max_lengths(opt.train_set)\n",
    "    training_set = CustomDataset(opt.train_set, opt.word2vec_path, max_sent_length, max_word_length)\n",
    "    training_generator = DataLoader(training_set, **training_params)\n",
    "    # validation set\n",
    "    test_set = CustomDataset(opt.val_set, opt.word2vec_path, max_sent_length, max_word_length)\n",
    "    test_generator = DataLoader(test_set, **test_params)\n",
    "\n",
    "    model = HierAttNet(opt.word_hidden_size, opt.sent_hidden_size, opt.batch_size, training_set.num_classes,\n",
    "                       opt.word2vec_path, max_sent_length, max_word_length)\n",
    "\n",
    "\n",
    "    if os.path.isdir(opt.log_path):\n",
    "        shutil.rmtree(opt.log_path)\n",
    "    os.makedirs(opt.log_path)\n",
    "    writer = SummaryWriter(opt.log_path)\n",
    "    # writer.add_graph(model, torch.zeros(opt.batch_size, max_sent_length, max_word_length))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=opt.lr)\n",
    "    best_loss = 1e5\n",
    "    best_epoch = 0\n",
    "    model.train()\n",
    "    num_iter_per_epoch = len(training_generator)\n",
    "    for epoch in range(opt.num_epoches):\n",
    "        for iter, (feature, label) in enumerate(training_generator):\n",
    "            if torch.cuda.is_available():\n",
    "                feature = feature.cuda()\n",
    "                label = label.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            model._init_hidden_state()\n",
    "            predictions = model(feature)\n",
    "            loss = criterion(predictions, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_metrics = get_evaluation(label.cpu().numpy(), predictions.cpu().detach().numpy(), list_metrics=[\"accuracy\"])\n",
    "            print(\"Epoch: {}/{}, Iteration: {}/{}, Lr: {}, Loss: {}, Accuracy: {}\".format(\n",
    "                epoch + 1,\n",
    "                opt.num_epoches,\n",
    "                iter + 1,\n",
    "                num_iter_per_epoch,\n",
    "                optimizer.param_groups[0]['lr'],\n",
    "                loss, training_metrics[\"accuracy\"]))\n",
    "            writer.add_scalar('Train/Loss', loss, epoch * num_iter_per_epoch + iter)\n",
    "            writer.add_scalar('Train/Accuracy', training_metrics[\"accuracy\"], epoch * num_iter_per_epoch + iter)\n",
    "        if epoch % opt.test_interval == 0:\n",
    "            model.eval()\n",
    "            loss_ls = []\n",
    "            te_label_ls = []\n",
    "            te_pred_ls = []\n",
    "            for te_feature, te_label in test_generator:\n",
    "                num_sample = len(te_label)\n",
    "                if torch.cuda.is_available():\n",
    "                    te_feature = te_feature.cuda()\n",
    "                    te_label = te_label.cuda()\n",
    "                with torch.no_grad():\n",
    "                    model._init_hidden_state(num_sample)\n",
    "                    te_predictions = model(te_feature)\n",
    "                te_loss = criterion(te_predictions, te_label)\n",
    "                loss_ls.append(te_loss * num_sample)\n",
    "                te_label_ls.extend(te_label.clone().cpu())\n",
    "                te_pred_ls.append(te_predictions.clone().cpu())\n",
    "            te_loss = sum(loss_ls) / test_set.__len__()\n",
    "            te_pred = torch.cat(te_pred_ls, 0)\n",
    "            te_label = np.array(te_label_ls)\n",
    "            test_metrics = get_evaluation(te_label, te_pred.numpy(), list_metrics=[\"accuracy\", \"confusion_matrix\"])\n",
    "            output_file.write(\n",
    "                \"Epoch: {}/{} \\nVal loss: {} Val accuracy: {} \\nVal confusion matrix: \\n{}\\n\\n\".format(\n",
    "                    epoch + 1, opt.num_epoches,\n",
    "                    te_loss,\n",
    "                    test_metrics[\"accuracy\"],\n",
    "                    test_metrics[\"confusion_matrix\"]))\n",
    "            print(\"Epoch: {}/{}, Lr: {}, Loss: {}, Accuracy: {}\".format(\n",
    "                epoch + 1,\n",
    "                opt.num_epoches,\n",
    "                optimizer.param_groups[0]['lr'],\n",
    "                te_loss, test_metrics[\"accuracy\"]))\n",
    "            writer.add_scalar('Val/Loss', te_loss, epoch)\n",
    "            writer.add_scalar('Val/Accuracy', test_metrics[\"accuracy\"], epoch)\n",
    "            model.train()\n",
    "            if te_loss + opt.es_min_delta < best_loss:\n",
    "                best_loss = te_loss\n",
    "                best_epoch = epoch\n",
    "                torch.save(model, opt.saved_path + os.sep + \"whole_model_han\")\n",
    "\n",
    "            # Early stopping\n",
    "            if epoch - best_epoch > opt.es_patience > 0:\n",
    "                print(\"Stop training at epoch {}. The lowest loss achieved is {}\".format(epoch, te_loss))\n",
    "                break\n",
    "    evaluate(opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        \"\"\"Implementation of the model described in the paper: Hierarchical Attention Networks for Document Classification\"\"\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--num_epoches\", type=int, default=5)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--word_hidden_size\", type=int, default=50)\n",
    "    parser.add_argument(\"--sent_hidden_size\", type=int, default=50)\n",
    "    parser.add_argument(\"--es_min_delta\", type=float, default=0.0,\n",
    "                        help=\"Early stopping's parameter: minimum change loss to qualify as an improvement\")\n",
    "    parser.add_argument(\"--es_patience\", type=int, default=5,\n",
    "                        help=\"Early stopping's parameter: number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique.\")\n",
    "    parser.add_argument(\"--train_set\", type=str, default=\"../binary_data/train.csv\")\n",
    "    parser.add_argument(\"--val_set\", type=str, default=\"../binary_data/test.csv\")\n",
    "    parser.add_argument(\"--test_set\", type=str, default=\"../binary_data/test.csv\")\n",
    "    parser.add_argument(\"--test_interval\", type=int, default=1, help=\"Number of epoches between testing phases\")\n",
    "    parser.add_argument(\"--word2vec_path\", type=str, default=\"./glove.6B.100d.txt\")\n",
    "    parser.add_argument(\"--log_path\", type=str, default=\"tensorboard/han_voc\")\n",
    "    parser.add_argument(\"--saved_path\", type=str, default=\"trained_models\")\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = get_args()\n",
    "train(opt)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
