{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAgWmYyCoqg_",
        "outputId": "38f96fbc-f44c-464e-b3c0-3ac1235fd29e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sH9CW6_otnS",
        "outputId": "e0e956ba-e658-42b4-a39c-357e75bda5b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/10701-project\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/'10701-project'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nQ_wQVCnl-9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
        "from collections import defaultdict\n",
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djiSJxqsn_Ic"
      },
      "outputs": [],
      "source": [
        "def get_volcabulary_and_list_words(data):\n",
        "    reviews_words = []\n",
        "    volcabulary = defaultdict(int)\n",
        "    for review in data[\"text\"]:\n",
        "        # review_words = Word2VecUtility.review_to_wordlist(review, remove_stopwords=True)\n",
        "        review_words = review.split()\n",
        "        reviews_words.append(review_words)\n",
        "        for word in review_words:\n",
        "            volcabulary[word] += 1\n",
        "    return volcabulary, reviews_words\n",
        "\n",
        "def get_reviews_word_index(reviews_words, volcabulary, max_words, max_length, word2index):\n",
        "    volcabulary = sorted(volcabulary.items(), key = lambda x : x[1], reverse = True)[:max_words]\n",
        "    reviews_words_index = [[start] + [(word2index[w] + index_from) if w in word2index else oov for w in review] for review in reviews_words]\n",
        "    # in word2vec embedding, use (i < max_words + index_from) because we need the exact index for each word, in order to map it to its vector. And then its max_words is 5003 instead of 5000.\n",
        "    # padding with 0, each review has max_length now.\n",
        "    reviews_words_index = sequence.pad_sequences(reviews_words_index, maxlen=max_length, padding='post', truncating='post')\n",
        "    return reviews_words_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRignMgKoRSV"
      },
      "outputs": [],
      "source": [
        "max_words = 5000\n",
        "max_length = 50\n",
        "\n",
        "# model training parameters\n",
        "batch_size = 32\n",
        "embedding_dims = 100\n",
        "nb_filter = 256\n",
        "filter_length = 3\n",
        "hidden_dims = 256\n",
        "meta_hidden_size = 512\n",
        "nb_epoch = 5\n",
        "num_classes = 2 # Change to 5 for 5-star\n",
        "\n",
        "index_from = 3\n",
        "start = 1\n",
        "# padding = 0\n",
        "oov = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkiC_Jx7oR2Y",
        "outputId": "9ab10609-ca2c-41bf-a138-0b840b42487a"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('binary_data/train.csv')\n",
        "val_data = pd.read_csv('binary_data/val.csv')\n",
        "test_data = pd.read_csv('binary_data/test.csv')\n",
        "data = pd.concat([train_data, val_data, test_data])\n",
        "print('get volcabulary...')\n",
        "volcabulary, reviews_words = get_volcabulary_and_list_words(data)\n",
        "print('get reviews_words_index...')\n",
        "\n",
        "\n",
        "f = open('./glove.6B.100d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = asarray(values[1:], dtype='float32')\n",
        "\tword_embed_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(word_embed_index))\n",
        "# create a weight matrix for words in training docs\n",
        "word_embed_matrix = zeros((len(vocabulary), embedding_dims))\n",
        "for id, word in enumerate(vocabulary.items()):\n",
        "\tword = word[0]\n",
        "\tword_embed_vector = word_embed_index.get(word)\n",
        "\tif word_embed_vector is not None:\n",
        "\t\tword_embed_matrix[i] = word_embed_vector\n",
        "\n",
        "reviews_words_index = get_reviews_word_index(reviews_words, volcabulary, max_words, max_length, word_embed_index)\n",
        "\n",
        "labels = data[\"stars\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK3Z-ZH7ozSA"
      },
      "outputs": [],
      "source": [
        "train_data = [reviews_words_index[:len(train_data)], train_data[\"dense_feats\"].values]\n",
        "valid_data = [reviews_words_index[len(train_data):len(train_data)+len(val_data)], val_data[\"dense_feats\"].values]\n",
        "test_data = [reviews_words_index[len(train_data)+len(val_data):], test_data[\"dense_feats\"].values]\n",
        "train_labels = [labels[:len(train_data)], data[\"sentiment_score\"].values[:len(train_data)]]\n",
        "valid_labels = [labels[len(train_data):len(train_data)+len(val_data)], data[\"sentiment_score\"].values[len(train_data): len(train_data)+len(val_data)]]\n",
        "test_labels = [labels[len(train_data)+len(val_data):], data[\"sentiment_score\"].values[len(train_data)+len(val_data):]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_input = Input(shape=(max_length,), name='text_input')\n",
        "meta_input = Input(shape=(dense_size,), name='meta_input')\n",
        "text_embed = Embedding(max_words + index_from, embedding_dims, \\\n",
        "                    input_length=max_length, weights=[word_embed_matrix])(text_input)\n",
        "# Text encoder\n",
        "text_conv = Convolution1D(filters=nb_filter,\n",
        "                        kernel_size=filter_length,\n",
        "                        padding='valid',\n",
        "                        activation='relu',\n",
        "                        strides=1)(text_embed)\n",
        "text_pool = MaxPooling1D(pool_size=2)(text_conv)\n",
        "text_flatten = Flatten()(text_pool)\n",
        "# Meta encoder\n",
        "meta_dense = Dense(meta_hidden_size, activation=\"relu\")(meta_input)\n",
        "meta_repr = Dropout(0.25)(meta_dense)\n",
        "\n",
        "# Concate\n",
        "concat = Concatenate([meta_repr, text_flatten])\n",
        "concat_dense = Dense(hidden_dims , activation=\"relu\")(concat)\n",
        "concat_repr = Dropout(0.25)(concat_dense)\n",
        "# MTL heads\n",
        "star_classify = Dense(num_classes, activation='softmax', name=\"star_classify\")(concat_repr)\n",
        "sent_classify = Dense(1, name = \"sent_classify\")(concat_repr)\n",
        "model = Model([text_input, meta_input], [star_classify, sent_classify])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu663lfHpouP",
        "outputId": "5bf899ab-9420-4200-9ad4-47ec86959fbb"
      },
      "outputs": [],
      "source": [
        "model.compile(loss={'star_classify': 'binary_crossentropy', 'sent_classify': 'mean_squared_error'}, \n",
        "              loss_weights={'star_classify': 0.9, 'sent_classify': 0.1}, \n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
        "              metrics={'star_classify': 'accuracy'})\n",
        "\n",
        "model.fit(train_data, train_labels, batch_size=batch_size,\n",
        "          epochs=nb_epoch,\n",
        "          validation_data=(valid_data, valid_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY0SOnJUrsIz"
      },
      "outputs": [],
      "source": [
        "preds = model.predict(test_data)\n",
        "print(confusion_matrix(test_labels,preds))\n",
        "print(\"Score:\",round(accuracy_score(test_labels,preds)*100,2))\n",
        "print(\"Classification Report:\",classification_report(test_labels,preds))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CNNModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
